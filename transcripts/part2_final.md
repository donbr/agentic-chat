# Part 2: RAG & Connectors ## [00:00:02 - 00:00:33] All right, Whiz, we continue the how to build ChatGPT build ChatGPT event series today. We thought we were event series today. We thought we were going to just do RAG. Turns out we're going to just do RAG. Turns out we're doing MCP too today. Isn't that right? doing MCP too today. &gt;&gt; That's right. &gt;&gt; Yeah. So, it turns out that there's some &gt;&gt; Yeah. So, it turns out that there's some evolution associated with the industry evolution associated with the industry where doing RAG and sort of using MCP. where doing RAG and sort of using MCP. There's quite a bit of an overlap. And we see that overlap exactly when we're we see that overlap exactly when we're connecting to out outside data sources ## [00:00:33 - 00:01:05] connecting to out outside data sources just like MCP was originally designed just like MCP was originally designed for. How nice. Yeah, it's uh you know for. Yeah, it's uh you know it's great because it helps us it's great because it helps us understand kind of a key part of the understand kind of a key part of the workflow which is that users are going workflow which is that users are going to have data. Uh people who are using to have data. Uh people who are using your application are going to have data your application are going to have data and they're going to want to add data. and they're going to want to add data. And I think this is something that is And I think this is something that is really uh you know helps us understand really uh you know helps us understand how to do that as an application how to do that as an application developer using Responses API for our developer using Responses API for our users. users. &gt;&gt; That's right. That's right. Yeah. So it ## [00:01:05 - 00:01:36] &gt;&gt; That's right. That's right. Yeah. So it really does feel like there's this real really does feel like there's this real step forward in, you know, we don't step forward in, you know, we don't really care about like what exactly is really care about like what exactly is MCP as much as like how do we use it to MCP as much as like how do we use it to like build cool apps similar to HTTP. We like build cool apps similar to HTTP. We don't really care how it does its magic don't really care how it does its magic under the hood. We just care that we can under the hood. We just care that we can sort of use APIs. Uh we can use them to sort of use APIs. Uh we can use them to get stuff. we can use them to change get stuff. we can use them to change stuff on the other side. And we see a ## [00:01:36 - 00:02:07] stuff on the other side. And we see a very similar thing with chat GBD very similar thing with chat GBD connectors and with MCP servers today. connectors and with MCP servers today. So really, really cool to see this come So really, really cool to see this come together in an event we thought was together in an event we thought was going to be a little bit more simple, going to be a little bit more simple, but turns out um you know, we we get but turns out um you know, we we get some of the latest buzzwords in here on some of the latest buzzwords in here on basic RAG. How cool. You're going to basic RAG. You're going to demo both for us today. Is that right? demo both for us today. &gt;&gt; Oh, you know it. &gt;&gt; All right. So simple &gt;&gt; All right. So simple one file RAG and take all my files, look one file RAG and take all my files, look through them, please. MCP server, it's through them, please. MCP server, it's going to be dope. I'm looking forward to ## [00:02:07 - 00:02:38] going to be dope. I'm looking forward to it. You ready to go? it. &gt;&gt; Uh, I could not be more ready. &gt;&gt; Let's &gt;&gt; Let's &gt;&gt; That's my responses to you. &gt;&gt; That's my responses to you. &gt;&gt; Uh, heard and uh, thank you for &gt;&gt; Uh, heard and uh, thank you for responding to my request. Let's get into responding to my request. Let's get into the Responses API and how to build chat the Responses API and how to build ChatGPT part two everybody. RAG and ChatGPT part two everybody. RAG and connectors. We're going to go ahead and connectors. We're going to go ahead and align ourselves to today's session. If align ourselves to today's session. If you've got questions along the way, ## [00:02:38 - 00:03:11] you've got questions along the way, smash the YouTube live chat or comment smash the YouTube live chat or comment on your viewing platform and we'll make on your viewing platform and we'll make sure to get you involved in the sure to get you involved in the conversation. Today we want to conversation. Today we want to understand a little bit about RAG a understand a little bit about RAG a little bit how it fits in to when we little bit how it fits in to when we want to build these very complex agentic want to build these very complex agentic applications and a little bit how we applications and a little bit how we want to start thinking not just from the want to start thinking not just from the developer perspective but also from the developer perspective but also from the user perspective. We want to take a look user perspective. We want to take a look at these ChatGPT connectors and learn ## [00:03:11 - 00:03:44] at these ChatGPT connectors and learn how to leverage them for retrieval. how to leverage them for retrieval. Whether we're doing simple single file Whether we're doing simple single file RAG or we're giving our application RAG or we're giving our application access to our entire set of documents on access to our entire set of documents on Google Drive for instance like is Google Drive for instance like is typical of many business enterprise typical of many business enterprise plans on ChatGPT plans on ChatGPT including ours at AMakerpace. And of including ours at AMakerpace. And of course, we want to build, ship, and course, we want to build, ship, and share this retrieval capability for our share this retrieval capability for our custom ChatGPT application with these custom ChatGPT application with these two pathways. So, just to be super two pathways. So, just to be super clear, these are the two paths. ## [00:03:44 - 00:04:18] clear, these are the two paths. One, we're going to do a simple file One, we're going to do a simple file upload and then the other we're going to upload and then the other we're going to leverage MCP. We're going to use model leverage MCP. We're going to use model context protocol sort of as the context protocol sort of as the connector. connector. So all this comes together quite nicely So all this comes together quite nicely in the end. From RAG two to connectors in the end. From RAG two to connectors we meet at the Responses API. What a we meet at the Responses API. What a beautiful part two of our series. So just to remind everybody what we're just to remind everybody what we're doing here. We're building ChatGPT step ## [00:04:18 - 00:04:48] doing here. We're building ChatGPT step by step. by step. So, if we go to the ChatGPT interface, So, if we go to the ChatGPT interface, we're going to be able to see a lot of we're going to be able to see a lot of different things that we can do. Today, different things that we can do. Today, we're going to focus on what happens we're going to focus on what happens when you click this little plus. You can when you click this little plus. You can go, you can look down at some of these go, you can look down at some of these connections that we can make. Um, if we connections that we can make. Um, if we zoom in a little bit, we can see this a zoom in a little bit, we can see this a bit better. Um, you can add simple files bit better. Um, you can add simple files or you can make these connections. Now, ## [00:04:48 - 00:05:19] or you can make these connections. Now, it turns out, as we'll see, we're going it turns out, as we'll see, we're going to actually build ship and share a to actually build ship and share a GitHub connector, a GitHub MCP GitHub connector, a GitHub MCP connector. And we can't see that here, connector. And we can't see that here, but if you click use connectors and you but if you click use connectors and you click more, you can see all of the click more, you can see all of the optional remote MCP servers that we can optional remote MCP servers that we can connect to. So, of course, we can do connect to. So, of course, we can do simple RAG, but we can also take it a simple RAG, but we can also take it a step further. Let's see how to do both step further. Let's see how to do both today. ## [00:05:19 - 00:05:50] today. But first, let's just root ourselves in But first, let's just root ourselves in the very foundational concept of the very foundational concept of retrieval and augmenting our generations retrieval and augmenting our generations with retrieved context. with retrieved context. You know, RAG was kind of developed to You know, RAG was kind of developed to solve a specific problem that is that solve a specific problem that is that these hallucinations we get from LLMs, these hallucinations we get from LLMs, the confident responses that are ## [00:05:50 - 00:06:21] the confident responses that are actually not true or we we can take a a actually not true or we we can take a a different definition that actually came different definition that actually came out just last week from OpenAI out just last week from OpenAI themselves. themselves. Plausible Plausible Plausible but false statements. This is from why but false statements. This is from why language models hallucinate came out language models hallucinate came out September 5th this year. September 5th this year. So this is kind of where the need for So this is kind of where the need for RAG begins and the need for retrieval RAG begins and the need for retrieval augmentation begins. ## [00:06:21 - 00:06:52] augmentation begins. And so the way we solve this with people And so the way we solve this with people when we're having conversations with when we're having conversations with them is we fact check them. The way we them is we fact check them. The way we solve this in general is we do solve this in general is we do factchecking with reference material. So factchecking with reference material. So we're going to retrieve some reference we're going to retrieve some reference material. We're going to augment our material. We're going to augment our prompt, our input, our context with that prompt, our input, our context with that reference material. And we're going to reference material. And we're going to generate better answers. So retrieval, generate better answers. So retrieval, augmented, generation. augmented, generation. And when we think about using RAG, we ## [00:06:52 - 00:07:24] And when we think about using RAG, we want to kind of zoom out for a second want to kind of zoom out for a second and think about, well, yeah, we're doing and think about, well, yeah, we're doing this as developers, but we want to kind this as developers, but we want to kind of do it in the service of the of do it in the service of the application that we're building. So, our application that we're building. So, our user is going to ask something to our user is going to ask something to our application. We can kind of deliver a application. We can kind of deliver a number of different number of different kinds of outputs depending on what we kinds of outputs depending on what we retrieve, depending on all of the things retrieve, depending on all of the things we put into our system prompt, our we put into our system prompt, our specific examples that we're putting in. ## [00:07:24 - 00:07:57] specific examples that we're putting in. Everything that we do to to sort of Everything that we do to to sort of engineer the context is going to engineer the context is going to contribute to, you know, really bad, contribute to, you know, really bad, good, or really really great outputs. good, or really really great outputs. And we always want to stay rooted in And we always want to stay rooted in that. But if we zoom in, you know, our that. But if we zoom in, you know, our user doesn't care, but we care as user doesn't care, but we care as developers. We can we can consider that developers. We can we can consider that there is a retrieval process and there there is a retrieval process and there is a generation process. And both of is a generation process. And both of these processes can be optimized on some these processes can be optimized on some level. And we want to do that even for ## [00:07:57 - 00:08:28] level. And we want to do that even for very simple file search applications. But really we should be starting with the retriever and we we should kind of the retriever and we we should kind of assume the generator is going to do what assume the generator is going to do what generator does. The generators will get generator does. The generators will get better over time. We have control over better over time. We have control over retrieval in a datacentric world. As we retrieval in a datacentric world. As we like to say, you know, as goes like to say, you know, as goes retrieval, so goes generation. retrieval, so goes generation. RAG is really just giving the LLM access RAG is really just giving the LLM access to stuff it wasn't trained on. new to stuff it wasn't trained on. new knowledge that's not included in the ## [00:08:28 - 00:09:00] knowledge that's not included in the corpus of documents that it actually saw corpus of documents that it actually saw during pre-training or even during during pre-training or even during mid-training and post training. mid-training and post training. Classically, we think about this as Classically, we think about this as dense vector retrieval dense vector retrieval being used to put the right stuff in being used to put the right stuff in context. We kind of think about it like context. We kind of think about it like we ask a question. We send that chunk we ask a question. We send that chunk that question to an embedding model. that question to an embedding model. We've already chunked up and embedded ## [00:09:00 - 00:09:31] We've already chunked up and embedded our data in a vector store uh storage our data in a vector store uh storage space for all of the chunked and space for all of the chunked and vectorized pieces of our data. And we're vectorized pieces of our data. And we're essentially looking for similar stuff, essentially looking for similar stuff, stuff that's similar to our question. stuff that's similar to our question. We set up a prompt template says We set up a prompt template says something like use the provided context something like use the provided context to answer the user's query. And to answer the user's query. And importantly, importantly, we say something like if you don't know ## [00:09:31 - 00:10:01] we say something like if you don't know the answer, the answer, don't answer. don't answer. Don't hallucinate. Say I don't know. As we like to say at AI Maker Space, we like to say at AI Maker Space, embrace the I don't know. That's what we embrace the I don't know. That's what we want our LLMs to do. And in fact, this want our LLMs to do. And in fact, this is exactly aligned with what we see is exactly aligned with what we see again from the paper that came out from again from the paper that came out from OpenAI this past week. They talk about OpenAI this past week. They talk about hallucinations being inevitable. Well, hallucinations being inevitable. Well, as they found, they're not because they ## [00:10:01 - 00:10:33] as they found, they're not because they can just say, "I don't know." Or they can just say, "I don't know." Or they can say, "I'm not answering that can say, "I'm not answering that question. question. Not going to lie to you, but I'm also Not going to lie to you, but I'm also not going to answer that question." And not going to answer that question." And so we want to make sure that our so we want to make sure that our applications are doing this. Presumably applications are doing this. Presumably OpenAI is doing this on the back end as OpenAI is doing this on the back end as we'll see through their Responses API we'll see through their Responses API for us. But in in sort of a handcrafted for us. But in in sort of a handcrafted way, we typically think about doing this way, we typically think about doing this in in in in in our applications. We're going to our applications. We're going to actually take that embedding ## [00:10:33 - 00:11:04] actually take that embedding representation out of the vector store. representation out of the vector store. We're going to convert it back to We're going to convert it back to natural language and we're going to put natural language and we're going to put those chunks of context into our prompt those chunks of context into our prompt into context. into context. This retrieval process, This retrieval process, it can be dense, it can be sparse. There it can be dense, it can be sparse. There can be lots of different ways to enhance can be lots of different ways to enhance this retrieval process. Uh, but this is this retrieval process. Uh, but this is sort of level zero very simplistic way sort of level zero very simplistic way of thinking about RAG in general before of thinking about RAG in general before we put the reference material in we put the reference material in context. Now, we're almost done with ## [00:11:04 - 00:11:34] context. Now, we're almost done with RAG. we have to still do the generation RAG. we have to still do the generation piece that is put all this stuff into piece that is put all this stuff into the chat model and get our you know the chat model and get our you know better answer here. So, we've got a better answer here. So, we've got a retrieval step, we've got a generation retrieval step, we've got a generation step. And if we think about step. And if we think about that app that we're building, right? that app that we're building, right? We've got the retriever, we've got the We've got the retriever, we've got the generator, we want to optimize that generator, we want to optimize that retrieval step. We want to consider that retrieval step. We want to consider that retrieval is the most important thing in retrieval is the most important thing in a datacentric world. Okay. So, consider ## [00:11:34 - 00:12:04] a datacentric world. Okay. So, consider the user for a second. The user only the user for a second. The user only wants a great output. They don't want wants a great output. They don't want any bad outputs, any mid outputs. I any bad outputs, any mid outputs. I don't want any of that. We're not the don't want any of that. We're not the user, but we care about our users. We have user empathy. We are the builders. As a developer, you are the builder. In other words, you're the server and the other words, you're the server and the user is the client. So, when you user is the client. So, when you implement RAG, your job is like super implement RAG, your job is like super focused on optimizing this retrieval ## [00:12:04 - 00:12:36] focused on optimizing this retrieval step. You're going to look at things step. You're going to look at things like which document, like which document, what are the chunk sizes, what are the what are the chunk sizes, what are the retrieval strategies and algorithms that retrieval strategies and algorithms that you're going to use to make sure that you're going to use to make sure that this output is super great for the user. this output is super great for the user. You know what docs you want, how you know what docs you want, how you want them chunked, how you want them want them chunked, how you want them retrieved, the kind of output that retrieved, the kind of output that that's going to provide the user. And that's going to provide the user. And really, you also want to make sure, really, you also want to make sure, again, back to that kind of system ## [00:12:36 - 00:13:07] again, back to that kind of system prompt level, you want to make sure that prompt level, you want to make sure that you're bounding the generation. You want you're bounding the generation. You want to make sure that if it's not super to make sure that if it's not super clear from the context, if it's not clear from the context, if it's not super clear from the retrieval, you super clear from the retrieval, you don't want to produce that answer. don't want to produce that answer. And so, you're doing a lot of work to And so, you're doing a lot of work to make sure that we produce that great make sure that we produce that great output. output. Effectively what we're doing is Effectively what we're doing is in simple kind of handcrafted RAG we're in simple kind of handcrafted RAG we're connecting users to the information that connecting users to the information that we know they need through our ## [00:13:07 - 00:13:37] we know they need through our application. application. Now there's a lot of simplification that Now there's a lot of simplification that can happen in 2025 in the fall here as can happen in 2025 in the fall here as we move up layers of abstraction. So we move up layers of abstraction. So let's outline what that looks like. For instance, if we think about the way instance, if we think about the way we've been doing RAG the past couple of we've been doing RAG the past couple of years, we actually don't really need to years, we actually don't really need to do all of this ourselves anymore. When do all of this ourselves anymore. When we're looking at kind of handcrafting ## [00:13:37 - 00:14:07] we're looking at kind of handcrafting with the Responses API, these pieces with the Responses API, these pieces right here, the embedding model, the right here, the embedding model, the vector store, spinning all this stuff up vector store, spinning all this stuff up ourselves, we actually don't really need ourselves, we actually don't really need to care about doing this. The responses to care about doing this. The Responses API actually does this for us. And API actually does this for us. And that's pretty cool because it kind of that's pretty cool because it kind of abstracts away a lot of the challenging abstracts away a lot of the challenging things that maybe we've faced in things that maybe we've faced in building our own custom RAG building our own custom RAG applications. and of course as we applications. and of course as we move up a layer of abstraction, it's ## [00:14:07 - 00:14:40] move up a layer of abstraction, it's important to note like that doesn't mean important to note like that doesn't mean you shouldn't care about what happens you shouldn't care about what happens inside anymore. For instance, there was inside anymore. For instance, there was a paper came out a couple weeks ago a paper came out a couple weeks ago called on the theoretical limitations of called on the theoretical limitations of embedding based retrieval. embedding based retrieval. We'll actually do RAG on this RAG paper We'll actually do RAG on this RAG paper very meta RAG classic AI maker space very meta RAG classic AI maker space classic LLM wizard stuff here. And the classic LLM wizard stuff here. And the idea is like actually as the world has idea is like actually as the world has changed and we've gone from simple IR to ## [00:14:40 - 00:15:10] changed and we've gone from simple IR to you know what we've come to know and you know what we've come to know and love as similarity based RAG the love as similarity based RAG the retrieval tasks we're actually supposed retrieval tasks we're actually supposed to accomplish now from reasoning to to accomplish now from reasoning to instruction following to coding these instruction following to coding these are actually not necessarily are actually not necessarily tasks that classic dense vector tasks that classic dense vector retrieval or dense retrieval as the retrieval or dense retrieval as the paper calls it can do a super great job paper calls it can do a super great job on so as we abstract let's just take on so as we abstract let's just take note that we shouldn't forget. So you note that we shouldn't forget. So you know just like just because we're using ## [00:15:10 - 00:15:41] know just like just because we're using LLMs it doesn't mean we don't have to LLMs it doesn't mean we don't have to care about what happens inside of them care about what happens inside of them anymore. It just kind of means that we anymore. It just kind of means that we probably have to care less especially in probably have to care less especially in a prototyping phase. So that seems to be a prototyping phase. So that seems to be true for RAG. Now when we're building true for RAG. Now when we're building with something like the Responses API with something like the Responses API and similarly we also can incorporate and similarly we also can incorporate this idea of connecting through model this idea of connecting through model context protocol through MCP. Now of context protocol through MCP. Now of course this shouldn't be a surprise if ## [00:15:41 - 00:16:13] course this shouldn't be a surprise if those of you that read the initial blog those of you that read the initial blog on MCP from November 2024 uh you may on MCP from November 2024 uh you may have seen this slide from us before. or have seen this slide from us before. or if you follow our channel, but the new if you follow our channel, but the new standard is for connecting AI assistance standard is for connecting AI assistance to where the data lives. And so, you to where the data lives. And so, you know, MCP, it's not about what it is, know, MCP, it's not about what it is, it's about how it's used. It's an open- it's about how it's used. It's an open- source standard. It's a protocol. It's a set of rules. Yes. Um, but what is it ## [00:16:13 - 00:16:44] set of rules. Yes. Um, but what is it for? It's for connecting. It's for for? It's for connecting AI applications. for connecting AI applications for connecting AI applications like cloud desktop, cloud code like cloud desktop, cloud code to data sources and tools like to data sources and tools like Google Drive, like Slack. Google Drive, like Slack. And it's about doing this connection of And it's about doing this connection of our apps to external systems in kind of ## [00:16:44 - 00:17:16] our apps to external systems in kind of a twoway birectional a twoway birectional flow flow flow because again we can sort of request and because again we can sort of request and we can also edit stuff on the other we can also edit stuff on the other side. For instance, if I'm messing with side. For instance, if I'm messing with my documents, maybe I want to actually my documents, maybe I want to actually be able to update something related to be able to update something related to those documents. those documents. And this birectionality is very similar And this birectionality is very similar to what we see in using classic APIs. So to what we see in using classic APIs. So using MCP, using MCP, using MCP, we can actually take our applications ## [00:17:16 - 00:17:46] we can actually take our applications and connect them, plug them in to data and connect them, plug them in to data sources, to tools, to content sources, to tools, to content repositories. repositories. Of course, MCP and the classic idea Of course, MCP and the classic idea of it is it's the USBC port for AI of it is it's the USBC port for AI applications. Again, two-way applications. Again, two-way standardized connection. standardized connection. Don't really care how it works. Just care that it does. Don't really care care that it does. Don't really care exactly what's going on inside. Just exactly what's going on inside. Just care how I can use it to charge my ## [00:17:46 - 00:18:17] care how I can use it to charge my phone, to move data, uh to do lots of phone, to move data, uh to do lots of different things. different things. And in fact, if you haven't looked at And in fact, if you haven't looked at the MCP website in a long time, I the MCP website in a long time, I literally just kind of read you this top literally just kind of read you this top three lines of the MCP three lines of the MCP protocol protocol protocol website directly. website directly. And this idea of connecting is what And this idea of connecting is what we're going to leverage today. This idea we're going to leverage today. This idea of doing it for oh look at that ChatGPT ## [00:18:17 - 00:18:47] of doing it for oh look at that ChatGPT is what we're going to leverage today. is what we're going to leverage today. Now importantly when we talk about building RAG we talk about kind of connecting about kind of connecting the information we know the users need. the information we know the users need. Um, this is maybe a better way to think Um, this is maybe a better way to think about when we're sort of giving cart about when we're sort of giving cart blanch access to all the files on our blanch access to all the files on our Google Drive, for instance. We're we're Google Drive, for instance. We're we're kind of doing more of a connecting users ## [00:18:47 - 00:19:17] kind of doing more of a connecting users to the information that we know they to the information that we know they want access to. want access to. It's not as curated. It's not as It's not as curated. handcrafted. there's more data in general, a larger there's more data in general, a larger scale. Fundamentally, that's going to scale. Fundamentally, that's going to dilute some of the context. Uh, but dilute some of the context. Uh, but users really do want this and users really do want this and certainly that's why we see it built out certainly that's why we see it built out as a feature in ChatGPT and our users as a feature in ChatGPT and our users are going to be no different in our ## [00:19:17 - 00:19:49] are going to be no different in our organization or more broadly in the organization or more broadly in the community and in the industry. community and in the industry. So all this comes together whether it's So all this comes together whether it's simple RAG or whether it's MCP in our simple RAG or whether it's MCP in our good friend the Responses API. So good friend the Responses API. So Responses API came out in March. In May of this year, they released some new of this year, they released some new tools and features in the Responses API, tools and features in the Responses API, adding some new built-in tools to the adding some new built-in tools to the Responses API are core API primitive for Responses API are core API primitive for building out agentic agent applications, ## [00:19:49 - 00:20:22] building out agentic agent applications, including oh, look at this. All remote including oh, look at this. All remote MCP servers, tools like image gen and MCP servers, tools like image gen and code interpreter, and improvements to code interpreter, and improvements to file search. So file search. So we can kind of think about this idea of we can kind of think about this idea of RAG. We can think about this idea of RAG. We can think about this idea of this retriever step. Uh really it's this retriever step. Uh really it's probably better visualized without this probably better visualized without this junk in the way because we can't really junk in the way because we can't really see the prompt that OpenAI is using. We ## [00:20:22 - 00:20:49] see the prompt that OpenAI is using. We don't really know what the prompt is on don't really know what the prompt is on the back end. We assume it's pretty the back end. We assume it's pretty good. Um, but it's not as clear as it is good. Um, but it's not as clear as it is when we do all this from scratch. And when we do all this from scratch. And this retrieval step, whether we're this retrieval step, whether we're retrieving documents directly retrieving documents directly from a single file from a single file through the Responses API or we're through the Responses API or we're retrieving, let's say, ## [00:20:53 - 00:21:24] information from a broader MCP server. MCP server. We're we're kind of doing the same We're we're kind of doing the same thing. We're going and getting thing. We're going and getting information. We're bringing it back into information. We're bringing it back into context. And so, uh, Whiz, I'd love to context. And so, uh, Whiz, I'd love to have a quick discussion with you and have a quick discussion with you and vibe on this point for a second. Uh, vibe on this point for a second. Uh, is it is it weird to say is it okay to is it is it weird to say is it okay to say that MCP is kind of doing RAG then? say that MCP is kind of doing RAG then? Um, how do you think about this? &gt;&gt; Uh, I mean, ## [00:21:24 - 00:21:56] &gt;&gt; Uh, I mean, no. I I don't think like it is a bit no. I I don't think like it is a bit weird of course, but it's not like the weird of course, but it's not like the weirdest. Yeah. weirdest. &gt;&gt; Uh MCP is like helping us do RAG, right? It's it's connecting us to data sources It's it's connecting us to data sources that we can then use to augment our our that we can then use to augment our our context. context. &gt;&gt; So, it's sort of it's sort of the &gt;&gt; So, it's sort of it's sort of the another layer of abstraction above kind another layer of abstraction above kind of doing RAG. It's kind it's kind of of doing RAG. It's kind it's kind of like um like um like um yeah it's kind of like giving us access ## [00:21:56 - 00:22:27] yeah it's kind of like giving us access to easily do RAG to easily do RAG amongst a bunch of different documents amongst a bunch of different documents like that's correct. Yes. like that's correct. &gt;&gt; Okay. Okay. &gt;&gt; I it's like it's like we need to connect &gt;&gt; I it's like it's like we need to connect to a data source, right? Typically to a data source, right? Typically that's going to be like a vector index. &gt;&gt; Yeah. &gt;&gt; Right. like uh like uh you know vector &gt;&gt; Right. like uh like uh you know vector database whatever your favorite one is database whatever your favorite one is pine cone or whatever. Uh what we're pine cone or whatever. Uh what we're saying is that instead now it's going to saying is that instead now it's going to be uh it's going to be handled through ## [00:22:28 - 00:22:59] be uh it's going to be handled through the uh MCP. So the MCP is going to in the uh MCP. So the MCP is going to in some ways replace that index. some ways replace that index. &gt;&gt; Okay. Okay. Yeah. So really it does all &gt;&gt; Okay. So really it does all come down to connecting whether we're come down to connecting whether we're talking about RAG talking about RAG and connecting to a single file and connecting to a single file and doing it in a handcrafted way and doing it in a handcrafted way and building our own custom vector database building our own custom vector database or we're talking about sort of being or we're talking about sort of being enabled to quickly and easily do RAG on enabled to quickly and easily do RAG on any of our documents. um we're sort of ## [00:22:59 - 00:23:31] any of our documents. um we're sort of connecting to connecting to the thing that we want to retrieve and the thing that we want to retrieve and we can connect directly to the file or we can connect directly to the file or we can connect directly to all the files we can connect directly to all the files and that's where sort of the MCP server and that's where sort of the MCP server comes in. Okay, so we've got a um comes in. Okay, so we've got a um a question in the chat from Manny here. a question in the chat from Manny here. uh connecting data on MCP then how uh connecting data on MCP then how secure is this and can we improve the secure is this and can we improve the security of this what's it look like in ## [00:23:31 - 00:24:02] security of this what's it look like in the Responses API today in terms of you the Responses API today in terms of you know your comfortability level shipping know your comfortability level shipping this kind of thing to production this kind of thing to production &gt;&gt; yes so it's actually not very secure uh &gt;&gt; yes so it's actually not very secure uh I I mean it that isn't to say that it's I I mean it that isn't to say that it's like you know there's no way for it to like you know there's no way for it to be secure but out of the box. Uh no, be secure but out of the box. Uh no, definitely not uh something that you definitely not uh something that you want to be uh thinking is ultra secure. want to be uh thinking is ultra secure. Uh however, ## [00:24:02 - 00:24:34] Uh however, you know, with things like OOTH, it is you know, with things like OOTH, it is the same level of security that we see the same level of security that we see from other tools with extra added fun, from other tools with extra added fun, you know, potential fail cases. At the you know, potential fail cases. At the end of the day, we're letting an end of the day, we're letting an autonomous semi-intelligent system autonomous semi-intelligent system access our data and uh so there's no access our data and uh so there's no &gt;&gt; there's no specific guarantee that &gt;&gt; there's no specific guarantee that that's going to be done in a way that is that's going to be done in a way that is most excellent every time, right? And so most excellent every time, right? And so I would what I implore people to do is I would what I implore people to do is think about this from the perspective of ## [00:24:34 - 00:25:04] think about this from the perspective of you know you know you know you wouldn't give just anyone access you wouldn't give just anyone access to your email. I hope, right? And so you to your email. And so you should think of it the same kind of way should think of it the same kind of way that we're we're giving access to the that we're we're giving access to the LLM uh as soon as we do that OOTH. And LLM uh as soon as we do that OOTH. And so there's two flavors of security. On one end, it's standard OOTH 2.0. We love it. On the other end, uh it's a it's a it. On the other end, uh it's a it's a intelligence mimicking intelligence mimicking uh system that has access to your ## [00:25:04 - 00:25:34] uh system that has access to your information. So information. So &gt;&gt; Okay. Okay. So I mean this is kind of an interesting So I mean this is kind of an interesting point because if we think about this point because if we think about this sort of birectional flow between sort of birectional flow between what we're asking for and and what what we're asking for and and what the LM is doing uh you can imagine it's the LM is doing uh you can imagine it's in there like manipulating the data in there like manipulating the data directly directly directly and um you know this this feels like and um you know this this feels like something we don't want it to do uh in ## [00:25:34 - 00:26:05] something we don't want it to do uh in general general general But certainly is part of the But certainly is part of the standard protocol sort of sort of like standard protocol sort of sort of like the you know I don't have to just the you know I don't have to just respond but I can do an update respond but I can do an update you know kind of kind of classic crud you know kind of kind of classic crud thing the thing the is this is this like something we need is this is this like something we need to watch out for uh today is is is if if to watch out for uh today is is is if if we're not just doing RAG which is we're not just doing RAG which is literally just retrieving it's just literally just retrieving it's just getting the information coming one way getting the information coming one way but this two-way thing is this is this ## [00:26:05 - 00:26:36] but this two-way thing is this is this is something we should be, you know, is something we should be, you know, wary of when we're building. wary of when we're building. &gt;&gt; Uh, &gt;&gt; Uh, yes. yes. &gt;&gt; Okay. Okay. &gt;&gt; Broadly speaking, yes. &gt;&gt; All right. Yeah. It feels like uh Yeah. So, it it feels similar to, you know, So, it it feels similar to, you know, thinking about how, you know, we have thinking about how, you know, we have some processes inside our company, for some processes inside our company, for instance, that uh that require access to instance, that uh that require access to accounts. And if if one were to just accounts. And if if one were to just retrieve that information, probably ## [00:26:36 - 00:27:06] retrieve that information, probably fine. Uh but if one were to also be able fine. Uh but if one were to also be able to generate stuff and change stuff and to generate stuff and change stuff and delete stuff, uh the D and CRUD there, delete stuff, uh the D and CRUD there, that that's terrifying actually, uh when that that's terrifying actually, uh when you think about our channel, you know, you think about our channel, you know, our our LinkedIn channel. And so, uh our our LinkedIn channel. And so, uh yeah, it's it's very much worth thinking yeah, it's it's very much worth thinking about. So, you get cart blanch access to about. So, you get cart blanch access to all your data, but then the LM has cart all your data, but then the LM has cart blanch access to all your data. Uh quite blanch access to all your data. Uh quite interesting. Okay. So uh we've got ## [00:27:06 - 00:27:37] interesting. Okay. So uh we've got another question here from uh from Black another question here from uh from Black Karant. Karant. Do we need an index of indices? So I think this is sort of in the context of think this is sort of in the context of using MCP as the ChatGPT connector using MCP as the ChatGPT connector here. Um if we're going to give it here. Um if we're going to give it access to all of our docs, do we need to access to all of our docs, do we need to provide anything or is it spinning that provide anything or is it spinning that up automatically? Uh this is the beauty, up automatically? Uh this is the beauty, right? Uh right? uh it's just doing this for us, right? I ## [00:27:37 - 00:28:08] uh it's just doing this for us, right? I mean, it's handling a lot of this stuff mean, it's handling a lot of this stuff that we're that we're that we're uh we're typically trying to to do just uh we're typically trying to to do just just for us, which is very very nice of just for us, which is very very nice of it, right? Uh the idea is that it, right? Uh the idea is that indeed we we we don't want to have to do indeed we we we don't want to have to do all of this extra stuff uh every time all of this extra stuff uh every time like a user looks at us, right? And so I like a user looks at us, right? And so I think that's the important piece here think that's the important piece here which is that you know we we want this ## [00:28:08 - 00:28:39] which is that you know we we want this to be seamless for our users and uh the to be seamless for our users and uh the Responses API gives us a way to do that Responses API gives us a way to do that without engineering like a dynamic RAG without engineering like a dynamic RAG you know super thing or whatever. Yeah. you know super thing or whatever. Right. &gt;&gt; Yeah. So, yeah. It's it's like uh the &gt;&gt; Yeah. It's it's like uh the one-click deployment RAG or something. one-click deployment RAG or something. Yeah. &gt;&gt; That's right. &gt;&gt; One click deployment super RAG. Okay. So, so if I just have one file then So, so if I just have one file then Okay. Um Okay. Um if if I just have one file, let's go to ## [00:28:39 - 00:29:11] if if I just have one file, let's go to this question. Um and it's in my Google this question. Um and it's in my Google Docs. Docs. Should I take it out of my Google Docs Should I take it out of my Google Docs and do do simple RAG and do a little and do do simple RAG and do a little more handcrafted approach direct or or more handcrafted approach direct or or just run it through the Responses API or just run it through the Responses API or or should I just uh give the MCP server or should I just uh give the MCP server access to it? Like what what are you access to it? Like what what are you doing today if you're trying to solve doing today if you're trying to solve this problem? Uh yeah, I mean uh if I if this problem? Uh yeah, I mean uh if I if I wanted to have it parse over a folder I wanted to have it parse over a folder or my calendar or etc or whatever blah ## [00:29:11 - 00:29:42] or my calendar or etc or whatever blah blah blah blah blah blah uh then I would blah blah blah blah blah uh then I would certainly want to use a connector, certainly want to use a connector, right? I would want to connect my uh my right? I would want to connect my uh my entire service. I if I'm using oneoff, entire service. I if I'm using oneoff, I'm just I'm just straight copy pasting I'm just I'm just straight copy pasting that stuff, right? Like uh that stuff, right? Like uh &gt;&gt; uh that's that's the beauty. uh if I if I want a little bit better than copy I want a little bit better than copy pasta, right? I'll I'll upload the PDF, pasta, right? I'll I'll upload the PDF, you know, like that's the uh it just you know, like that's the uh it just allowing me to to to make the decision allowing me to to to make the decision when I need to as opposed to forcing me ## [00:29:42 - 00:30:13] when I need to as opposed to forcing me to make it kind of uh you know uh to make it kind of uh you know uh ahead of time or or or thinking so so ahead of time or or or thinking so so much about it, right? Yeah, much about it, right? Yeah, &gt;&gt; I would say that's what what is most &gt;&gt; I would say that's what what is most useful about this different these useful about this different these different patterns. different patterns. &gt;&gt; Simplest solution is the best. &gt;&gt; That's right. &gt;&gt; Don't need more horsepower than you &gt;&gt; Don't need more horsepower than you need. Have you actually tapped out the need. Have you actually tapped out the horsepower that you have? And then yeah, horsepower that you have? And then yeah, going back to being datacentric and going back to being datacentric and literally copying literally copying your data, pasting it in to the ## [00:30:13 - 00:30:45] your data, pasting it in to the generator that is also RAG. And so generator that is also RAG. And so people can people can still do that people can people can still do that &gt;&gt; even though we have this crazy level of &gt;&gt; even though we have this crazy level of abstraction that we can work at super abstraction that we can work at super easily now. And so easily now. And so &gt;&gt; so all this is abstracted from us uh for &gt;&gt; so all this is abstracted from us uh for those of us that are used to like those of us that are used to like writing our own prompts and maybe you writing our own prompts and maybe you can maybe you can show us what we what can maybe you can show us what we what we do know in the demo here in just a we do know in the demo here in just a moment. But like we don't really have moment. But like we don't really have access to the prompts in the same way we ## [00:30:45 - 00:31:15] access to the prompts in the same way we do when we handcraft. And this is where do when we handcraft. And this is where we start to get into this classic space we start to get into this classic space as technology abstracts and as we we can as technology abstracts and as we we can build at higher and higher levels that build at higher and higher levels that we no longer have access to the stuff at we no longer have access to the stuff at lower levels. So um what do we know lower levels. So um what do we know about the prompts being used for file about the prompts being used for file search about the prompts being used for search about the prompts being used for tool descriptions in the remote MCP tool descriptions in the remote MCP servers? Do we know anything at all? servers? &gt;&gt; Uh no. Uh, I mean we we we can infer ## [00:31:15 - 00:31:48] &gt;&gt; Uh no. Uh, I mean we we we can infer things or we can get stuff from the uh, things or we can get stuff from the uh, you know, we can we can kind of like you know, we can we can kind of like request them, but the prompts are request them, but the prompts are relatively relatively relatively uh uh they're straightforwardly just they're straightforwardly just not not like important to us is what I'm not not like important to us is what I'm going to say, right? So they're they're going to say, right? So they're they're they're being they're being changed and modified on the back end uh changed and modified on the back end uh to be most compatible with whatever the to be most compatible with whatever the task is. Uh and that that task is ## [00:31:48 - 00:32:18] task is. Uh and that that task is separate to like the task that we're separate to like the task that we're requesting. Uh and so you can requesting. Uh and so you can imagine that they're doing the same imagine that they're doing the same amount of work amount of work &gt;&gt; behind the scenes as as we might. &gt;&gt; behind the scenes as as we might. &gt;&gt; Okay. Okay. So, it's still going to &gt;&gt; Okay. So, it's still going to serve as a great baseline if we wanted serve as a great baseline if we wanted to go out and produce our own to go out and produce our own application. If we wanted to go out and application. If we wanted to go out and handcraft, this is going to be a great handcraft, this is going to be a great way similar to how we would use an open way similar to how we would use an OpenAI model off the shelf. We don't exactly AI model off the shelf. We don't exactly know what's going on, but we can use it know what's going on, but we can use it to say, can I solve this problem? And to say, can I solve this problem? And then if you really want to solve the ## [00:32:18 - 00:32:50] then if you really want to solve the problem, then start getting super problem, then start getting super detailed about every little aspect of detailed about every little aspect of it. Um, to the extent that that allows it. Um, to the extent that that allows you to optimize performance and you to optimize performance and efficiency, etc., etc., etc., for your efficiency, etc., etc., etc., for your organization. organization. &gt;&gt; That's right. &gt;&gt; Okay. Very cool. Well, then I think &gt;&gt; Okay. Well, then I think we're ready to go ahead and get to the we're ready to go ahead and get to the demo today. Whiz the building of demo today. Whiz the building of connectors. We're going to break down connectors. We're going to break down into two parts. As mentioned, you're into two parts. As mentioned, you're going to look at the file search and going to look at the file search and we're going to look at remote MCP we're going to look at remote MCP servers. So, we've got the paper teed up servers. So, we've got the paper teed up and then as mentioned, if you sort of ## [00:32:50 - 00:33:24] and then as mentioned, if you sort of double click in on use connectors, you double click in on use connectors, you know, everybody can kind of see all the know, everybody can kind of see all the connectors pop up. We're going to use connectors pop up. We're going to use one today. We're gonna use GitHub. I'm one today. I'm excited for this one. Handing it over to excited for this one. Handing it over to you, Whiz Connectors with RAG and MCP you, Whiz Connectors with RAG and MCP demo. Buckle up, everybody. Part two. demo. &gt;&gt; We're connecting out here. Uh, okay. So, yes, let me share the old screen arena yes, let me share the old screen arena here. There we go. And, uh, we're going here. And, uh, we're going to look at a couple different things. to look at a couple different things. The first thing we're going to look at The first thing we're going to look at is uh, you know, ChatGPT. So, easy ## [00:33:24 - 00:33:54] is uh, you know, ChatGPT. So, easy enough. If we want to use connectors, we enough. If we want to use connectors, we can click use connectors. Then we can can click use connectors. Then we can click on our sources. You can see that I click on our sources. You can see that I have already uh added my Google have already uh added my Google calendar. If I wanted to add others, I calendar. If I wanted to add others, I would just do this here. You can also would just do this here. You can also connect, of course, to GitHub. Uh but connect, of course, to GitHub. Uh but we're we're connecting the calendar. You can ask questions like, yo, dog, you can ask questions like, yo, dog, you know, uh what uh event do I have on know, uh what uh event do I have on Friday? you know, and then it's going to ## [00:33:54 - 00:34:24] Friday? you know, and then it's going to do its thing, use its use its int do its thing, use its use its int incredible intelligence to determine incredible intelligence to determine with the connector what I've got going with the connector what I've got going on Friday. Uh, you can trust that it's on Friday. Oh, look, it found one doing this. Oh, look, it found one events in Google calendar. What a Chad. events in Google calendar. Uh, and so then we're we're going the Uh, and so then we're we're going the rest of the way now. Uh, and we see ah, rest of the way now. Uh, and we see ah, there's a GPU engineering meetups there's a GPU engineering meetups fundamental of GPU orchestration that fundamental of GPU orchestration that I'm going to. That seems pretty cool. I I'm going to. I like that. Uh, and so, uh, I'm going to ## [00:34:24 - 00:34:56] like that. Uh, and so, uh, I'm going to it as well. So, that's great. Uh, so it as well. Uh, so we're going to look at how we would do we're going to look at how we would do this ourselves in the notebook. So, this ourselves in the notebook. So, we're going to go to our how to build we're going to go to our how to build Jat ChatGPT uh, repository. We're going to Jat ChatGPT uh, repository. We're going to go to uh, Responses API data and go to uh, Responses API data and connectors notebook and uh, we're going connectors notebook and uh, we're going to look through how we do this. Uh, now to look through how we do this. Uh, now the first thing we're going to do before the first thing we're going to do before that is we're going to look at how we do that is we're going to look at how we do this part which is adding some files. this part which is adding some files. Maybe I want to add this paper. Uh, you know, I can add it here. Then ask know, I can add it here. Then ask questions. Uh, why uh dense vector ## [00:34:56 - 00:35:28] questions. Uh, why uh dense vector embeddings so bad, bro? You know, and embeddings so bad, bro? You know, and then I can I can send that request uh then I can I can send that request uh once it's done uploading. Uh, and the once it's done uploading. Uh, and the idea is here that of course uh then I idea is here that of course uh then I can ask ask questions about it. Okay, so can ask ask questions about it. Okay, so we get it, right? There you go. we get it, right? Everyone's happy. Uh, we get it. Uh now Everyone's happy. Uh now in the Responses API this is actually in the Responses API this is actually kind of like just as easy. Uh so we'll kind of like just as easy. Uh so we'll start our client and then we'll have a start our client and then we'll have a create file uh which is going to give us ## [00:35:28 - 00:36:01] create file uh which is going to give us the file ID that we need. Now the reason the file ID that we need. Now the reason we have the file ID uh is because we we have the file ID uh is because we want to be able to like track uh what want to be able to like track uh what the uh you know what the actual the uh you know what the actual resource is called in OpenAI's servers resource is called in OpenAI's servers right because it's not just going to right because it's not just going to save the file name there may be many save the file name there may be many files with the same name so it gives us files with the same name so it gives us a unique identifier which we want to a unique identifier which we want to which we want to get and the way that we which we want to get and the way that we get that is we simply just upload our uh ## [00:36:01 - 00:36:32] get that is we simply just upload our uh our file, right? Uh in this case, we our file, right? Uh in this case, we have, of course, this this full path uh have, of course, this this full path uh to to some DATMSS. Uh and then you're to to some DATMSS. Uh and then you're going to be able to upload that file. going to be able to upload that file. Very cool. Then we can create a vector Very cool. Then we can create a vector store with it. The way we do this crazy store with it. The way we do this crazy it's going to seem, but we use it's going to seem, but we use client. vectors. create client. vectors. create and then it creates a vector store for and then it creates a vector store for us. Very cool. Love to see it. Uh us. Uh finally with that vector store created finally with that vector store created we can then upload. Now if you've been ## [00:36:32 - 00:37:04] we can then upload. Now if you've been with us for a little while uh this is with us for a little while uh this is going to be a very familiar seeming going to be a very familiar seeming format right uh we create the vector format right uh we create the vector store then we upload stuff to it right store then we upload stuff to it right or we upsert vectors right uh same same or we upsert vectors right uh same same here. Then of course uh we process the here. Processing the file is basically file. Processing the file is basically just going to make sure that it's done just going to make sure that it's done processing before we try to use it. If processing before we try to use it. If it's not, we're gonna get yelled at. No one likes being yelled at. Very cool. Okay. Then uh we have the ability to ask ## [00:37:04 - 00:37:37] Okay. Then uh we have the ability to ask questions about it. We can ask questions questions about it. We can ask questions about it using our vector store ID with about it using our vector store ID with our tool of type file search. You can our tool of type file search. You can see like we can ask questions uh you see like we can ask questions uh you know what is deep research by OpenAI? know what is deep research by OpenAI? Deep research is blah blah blah blah Deep research is blah blah blah blah blah blah blah. We can look at some blah blah blah. We can look at some searches for it. uh we can look at some uh you know key capabilities of it uh you know key capabilities of it uh using you know the actual search being using you know the actual search being included in the response. We can limit included in the response. We can limit how many responses we get uh because how many responses we get uh because this is going to help us have lower this is going to help us have lower latency. And then of course we can add ## [00:37:37 - 00:38:07] latency. And then of course we can add whatever we've got in our local file in whatever we've got in our local file in the data directory in this repo. We can the data directory in this repo. We can add that to our uh to our vector store, add that to our uh to our vector store, right? And then we can do right? And then we can do multi-doccument search uh which is multi-doccument search uh which is basically the same thing, right? It's basically the same thing, right? It's just going to be the same tool. Uh this is the this is kind of the beauty, is the this is kind of the beauty, right? uh we don't have to spend our right? uh we don't have to spend our engineering cycles building the system. engineering cycles building the system. The system is you know leveraged through The system is you know leveraged through the Responses API directly. So that's ## [00:38:07 - 00:38:41] the Responses API directly. So that's great and we get by the way great great and we get by the way great response like this is dope. Uh great response like this is dope. Uh great stuff. Here's all the things you can stuff. It's a bunch of stuff. That's upload. That's great. For MCPN connectors this is even great. For MCPN connectors this is even easier honestly. Uh and the reason it's easier honestly. Uh and the reason it's even easier is because it is uh even easier is because it is uh effectively uh it is a uh it is just a effectively uh it is a uh it is just a single call right uh to the uh ## [00:38:41 - 00:39:13] single call right uh to the uh you know you to the API that allows us you know you to the API that allows us to use this connector. So when I look at to use this connector. So when I look at my connectors here, I'm just going to my connectors here, I'm just going to scroll back down. scroll back down. uh when we look at our connectors here, uh when we look at our connectors here, you're going to see that we have two you're going to see that we have two types. We have one which is this kind of types. We have one which is this kind of uh actual literal connector, right, uh actual literal connector, right, which is like a specific name. The which is like a specific name. The connector you can think of is like connector you can think of is like OpenAI's own MCP for this particular OpenAI's own MCP for this particular tool. So this is like for calendar, tool. Calendar doesn't have like a right? Calendar doesn't have like a publicly hosted MCP that you can just ## [00:39:13 - 00:39:45] publicly hosted MCP that you can just use. And so this is their version of use. Now, it's going to need this that. Now, it's going to need this authorization token, right, from O2. And authorization token, right, from O2. And we're going to talk about this when we we're going to talk about this when we start talking about end to end, how to start talking about end to end, how to incorporate this into like an actual incorporate this into like an actual workflow, right? That users can get that workflow, right? That users can get that classic Google screen popping up, right? classic Google screen popping up, right? That you can click your Google account That you can click your Google account and sign in. Uh, but for now, we're just and sign in. Uh, but for now, we're just going to use this uh, you know, this OAS going to use this uh, you know, this OAS 2 playground. Uh, I'm leaking my my 2 playground. Uh, I'm leaking my my token here. So uh you know if you if you ## [00:39:45 - 00:40:15] token here. So uh you know if you if you really want to check my calendar very really want to check my calendar very quickly uh now is your time. But the quickly uh now is your time. But the idea is that we have this uh to get our idea is that we have this uh to get our o token and then we can use that token o token and then we can use that token to connect to our calendar and we can to connect to our calendar and we can ask things like you know hey uh you know ask things like you know hey uh you know uh what events do I have in my Google uh what events do I have in my Google Google calendar for Friday and we get Google calendar for Friday and we get the same response that we got from chat the same response that we got from ChatGPT right which is very cool. Uh and GPT right which is very cool. Uh and this is the idea guys. This is re this is the idea guys. This is re realistically like this is the whole realistically like this is the whole thing. Uh we can also do things like ## [00:40:15 - 00:40:46] thing. Uh we can also do things like searching the whole week, right? Finding searching the whole week, right? Finding the busiest days. So if we look at my my the busiest days. So if we look at my my uh my very personal nonwork calendar, uh my very personal nonwork calendar, you can see got a lot of availability, you can see got a lot of availability, right? Excellent. Uh you know uh let's right? Uh you know uh let's not worry about what the work calendars not worry about what the work calendars look like, but you know this is great. look like, but you know this is great. Uh and then finally we have our MCP. So MCP again this is a single line right MCP again this is a single line right it's a single line it you know this is ## [00:40:46 - 00:41:18] it's a single line it you know this is technically many lines but you get the technically many lines but you get the point right this is a this is still point right this is a this is still considered a single line uh the idea is considered a single line uh the idea is that we can just point to whatever that we can just point to whatever remote hosted MCP we have we can provide remote hosted MCP we have we can provide authentication if we need it in this authentication if we need it in this case we don't and that's really it case we don't and that's really it like it's it's that easy to connect to like it's it's that easy to connect to MCP with the Responses API so you can MCP with the Responses API so you can imagine right if you have many users and imagine right if you have many users and they have an MCP server that they're they have an MCP server that they're excited about that you can let them excited about that you can let them access it through through you know access it through through you know through this workflow uh and that's ## [00:41:18 - 00:41:51] through this workflow uh and that's all you have to do right in this case all you have to do right in this case we're using git get MCP which is we're using git get MCP which is basically going to uh slurp up docs from basically going to uh slurp up docs from uh from GitHub on the tick token library uh from GitHub on the tick token library and then we can ask how it works and it and then we can ask how it works and it gives us a uh a response but this is all gives us a uh a response but this is all done again through the Responses API we done again through the Responses API we don't have to build any MCP server we don't have to build any MCP server we don't have to build any connectors, don't have to build any connectors, right? We're just leveraging this right? We're just leveraging this through the Responses API because that through the Responses API because that API uh is ultimately like the this API uh is ultimately like the this the central powerhouse of our uh of our ## [00:41:51 - 00:42:23] the central powerhouse of our uh of our entire uh you know uh system. It it's entire uh you know uh system. It it's like the the nexus through which all like the the nexus through which all things are run. Uh the long and short of things are run. Uh the long and short of it is this it is this it is this with single requests right uh we can use with single requests right uh we can use a a system like RAG MCP or connectors a a system like RAG MCP or connectors and this is how we get this pleasant and this is how we get this pleasant experience that you see in ChatGPT with experience that you see in ChatGPT with our connectors right so uh there's a lot our connectors right so uh there's a lot of like application side development uh ## [00:42:23 - 00:42:53] of like application side development uh that is missing here so how do we that is missing here so how do we actually handle the OOTH uh 2.0 you actually handle the OOTH uh 2.0 you know, seamlessly, you know, how do we know, seamlessly, you know, how do we make sure that users can add things in a make sure that users can add things in a way that's not errorprone? Uh, but the way that's not errorprone? Uh, but the crux of it, the actual core the crux of it, the actual core technology, real simple, uh, technology, real simple, uh, straightforward API requests. There you straightforward API requests. There you go. I've been the whiz for this day's go. I've been the whiz for this day's technical demo. Uh, before I pass you technical demo. Uh, before I pass you guys back to Greg, I got to remind you guys back to Greg, I got to remind you to like and subscribe, you know, ring ## [00:42:53 - 00:43:24] to like and subscribe, you know, ring the bell notification. We're here every the bell notification. We're here every Wednesday. And with that, we'll send you Wednesday. And with that, we'll send you back to Dr. Greg. Let's go. back to Dr. Greg. &gt;&gt; All right. Thanks, Whiz. And it is time &gt;&gt; All right. And it is time to uh conclude today's discussion. So, to uh conclude today's discussion. So, if you have questions, we do have plenty if you have questions, we do have plenty of time. We're working on more of time. We're working on more introductory level content like this as introductory level content like this as we get people ramped into building chat we get people ramped into building ChatGPT on their own. It's going to get ChatGPT on their own. It's going to get complex soon enough, guys, but for complex soon enough, guys, but for today, raging connectors, pretty ## [00:43:24 - 00:43:57] today, raging connectors, pretty straightforward. Uh what we saw today is straightforward. Uh what we saw today is we saw that part two allows us to build we saw that part two allows us to build RAG connectors with MCP RAG connectors with MCP or just simple file search. We can or just simple file search. We can connect to all sorts of different tools, connect to all sorts of different tools, all sorts of different servers super all sorts of different servers super easily. This is way easily. This is way beyond what we've been able to do in the beyond what we've been able to do in the past very very easily. So it's great to past very very easily. So it's great to see this integration of MCP in a simple ## [00:43:57 - 00:44:27] see this integration of MCP in a simple easy to use API setup. So you know if easy to use API setup. So you know if you can dream it you can build it easier you can dream it you can build it easier than ever. Doing RAG or leveraging these than ever. Doing RAG or leveraging these MCP servers MCP servers is really about the same thing. It's is really about the same thing. It's about retrieving the information that we about retrieving the information that we need and putting it into context. It's need and putting it into context. It's about engineering that context. It's about making connections to the systems about making connections to the systems where data lives. And so Whiz, I'm going where data lives. And so Whiz, I'm going to kick off uh discussion today, QA to kick off uh discussion today, QA today uh with a couple of questions for ## [00:44:27 - 00:44:59] today uh with a couple of questions for you. you. So we've looked at the Responses API So we've looked at the Responses API over the last two sessions. over the last two sessions. Um is this like a gold standard? Uh we teach langraph in class, but what do you teach langraph in class, but what do you think about the Responses API? Is this think about the Responses API? Is this is this the right tool for people just is this the right tool for people just getting started? getting started? &gt;&gt; I mean, it's it's a good API, right? I mean, it it it does what it does. Uh so that's the that's the thing. Uh ## [00:45:03 - 00:45:34] yeah, I mean the idea is effectively it's I I I think it's meant for it's I I I think it's meant for different you know audience, right? Uh different you know audience, right? Uh so Responses API is something that like so Responses API is something that like lang graph or lang chain will have a lang graph or lang chain will have a connector for right and allow us to connector for right and allow us to compose together into different things compose together into different things versus using it directly but to get versus using it directly but to get started I mean it's great it works you started I mean it's great it works you can build cool apps with it can build cool apps with it what what more in life do you need but what what more in life do you need but you can't see the prompts and so like you can't see the prompts and so like this is where this is where the open this is where this is where the open source tools source tools kind of come into play like like we're ## [00:45:34 - 00:46:04] kind of come into play like like we're seeing this it's not low code of seeing this it's not low code of But it's it's sort of lower code. That's But it's it's sort of lower code. That's right. right. &gt;&gt; We're seeing we're seeing this lower &gt;&gt; We're seeing we're seeing this lower code now like uh you know code now like uh you know opportunity space open up for opportunity space open up for &gt;&gt; that's right &gt;&gt; that's right &gt;&gt; for builders to pick up tools and &gt;&gt; for builders to pick up tools and get get going super duper quickly. Um get get going super duper quickly. Um yeah just it just kind of works. It is open source but as we start connecting ## [00:46:04 - 00:46:35] open source but as we start connecting to you know different parts of the open to you know different parts of the OpenAI ecosystem it gets less open the AI. AI ecosystem it gets less open the AI. &gt;&gt; That's right. Um so uh Manny asks if we are using this Responses API can we easily plug in like Responses API can we easily plug in like a lang to it or wand a lang to it or wand to get the stuff we want about what's to get the stuff we want about what's actually going on as we're operating. actually going on as we're operating. &gt;&gt; If we yeah if we build it ourselves on ## [00:46:35 - 00:47:07] &gt;&gt; If we yeah if we build it ourselves on our end of course we can do that. Yes. our end of course we can do that. uh if we're if we're not building uh if we're if we're not building something on our end. No. So like as in something on our end. So like as in I don't think you can inject I don't think you can inject the the I I I don't think you can inject that I I I don't think you can inject that during the flow of work on OpenAI's side during the flow of work on OpenAI's side but I do think that you can inject that but I do think that you can inject that uh well I know you can inject it if uh well I know you can inject it if you're building the application from you're building the application from from the ground up. So, what I would say from the ground up. So, what I would say is if you're a builder, yes. If you're a ## [00:47:07 - 00:47:42] is if you're a builder, yes. If you're a user, uh, unclear. user, uh, unclear. &gt;&gt; Okay. Okay. &gt;&gt; Yeah. &gt;&gt; All right. And um you know as as I think &gt;&gt; All right. And um you know as as I think about kind of the future of RAG and the about kind of the future of RAG and the future of MCP here like I'm kind of future of MCP here like I'm kind of reminded of reminded of you know the Apple charger refusing to change to micro change to USBC and then ultimately ## [00:47:42 - 00:48:12] change to USBC and then ultimately they they they Yeah. Yeah. &gt;&gt; Are you expecting something similar? Like like it just feels like um soon Like like it just feels like um soon enough nobody will say RAG and we'll enough nobody will say RAG and we'll just hit MCP servers or something. Um just hit MCP servers or something. Um yeah, yeah, yeah, I think that might be true. I think that might be true. &gt;&gt; Yeah. You know, I I mean I I don't think &gt;&gt; Yeah. You know, I I mean I I don't think it's I I mean it's technically not true. it's I I mean it's technically not true. Okay. It's technically Okay. It's technically &gt;&gt; sure there will be &gt;&gt; sure there will be &gt;&gt; that's not the way it works. Blah blah &gt;&gt; that's not the way it works. Blah blah blah. Uh but like MCP is gonna be part ## [00:48:12 - 00:48:44] blah. Uh but like MCP is gonna be part of it if it's not uh of it if it's not uh &gt;&gt; like part of the equation &gt;&gt; like part of the equation &gt;&gt; for people who want to attach their own &gt;&gt; for people who want to attach their own data. It's a it's a good thing for that, data. Like that is what it's good at. right? &gt;&gt; That's what it's for. And we just got to &gt;&gt; That's what it's for. And we just got to deal with the whole like don't delete it deal with the whole like don't delete it problem. problem. &gt;&gt; That's right. &gt;&gt; It's almost like similar to the don't &gt;&gt; It's almost like similar to the don't lie to me problem. Now we're on the lie to me problem. Now we're on the don't delete it problem. don't delete it problem. &gt;&gt; Yes. Okay. And yeah. So, ## [00:48:44 - 00:49:14] &gt;&gt; Yeah. Okay. And yeah. So, all right. I mean that that makes sense all right. I mean that that makes sense to me. So, so yeah, I mean I'm kind of to me. So, so yeah, I mean I'm kind of thinking of RAG as micro USB and then thinking of RAG as micro USB and then you know MCP is like the shiny new USBC you know MCP is like the shiny new USBC and it's faster and it's better and it's and it's faster and it's better and it's &gt;&gt; I think it's I think it's better to &gt;&gt; I think it's I think it's better to think of RAG as a process by which we think of RAG as a process by which we augment the uh context window of the augment the uh context window of the LLM, right? And so MCP helps augment the LLM, right? And so MCP helps augment the context window but like that that means context window but like that that means it's RAG, right? ## [00:49:14 - 00:49:48] it's RAG, right? leveraging the pattern of rack. It I I would say like MCP helps helps us would say like MCP helps helps us replace things like our retrieval replace things like our retrieval pipelines, but it doesn't remove RAG. pipelines, but it doesn't remove RAG. RAG still exists. Rag's still dope. We love RAG. Uh because love RAG. Uh because MCP is like the retriever pipeline, MCP is like the retriever pipeline, right? It's not doing it's not right? It's not doing it's not augmenting the the LLM uh you augmenting the the LLM uh you know uh by itself. We still have to have know uh by itself. We still have to have some pipeline that takes whatever we got some pipeline that takes whatever we got from the MCP server and puts it into our ## [00:49:48 - 00:50:18] from the MCP server and puts it into our uh into our uh into our uh application. uh application. &gt;&gt; Yeah. Okay. So then so then if I'm if &gt;&gt; Yeah. So then so then if I'm if I'm just getting into AI engineering, I'm just getting into AI engineering, let's say I'm watching this video, I'm let's say I'm watching this video, I'm like I'm like I think this is this is a like I'm like I think this is this is a good series for me to get into AI good series for me to get into AI engineering. engineering. &gt;&gt; Should I care about building retrieval &gt;&gt; Should I care about building retrieval pipelines right now? uh or should I care pipelines right now? uh or should I care about leveraging MCP servers? about leveraging MCP servers? &gt;&gt; Yeah, you should absolutely care about &gt;&gt; Yeah, you should absolutely care about building retrieval pipelines. I mean, building retrieval pipelines. I mean, that's one of the most important things ## [00:50:18 - 00:50:49] that's one of the most important things that you can do, right? You know, the that you can do, right? You know, the data. data. &gt;&gt; Uh you you know &gt;&gt; Uh you you know &gt;&gt; Yeah, that's the problem with this, &gt;&gt; Yeah, that's the problem with this, right? Because you're kind of handing right? Because you're kind of handing off uh your your expertise, your your off uh your your expertise, your your subject, your SME knowledge of the data subject, your SME knowledge of the data to MCP server. Um to MCP server. Um &gt;&gt; I think they're just so distinct that I &gt;&gt; I think they're just so distinct that I think having them conflated them like think having them conflated them like this is it it just I think it might this is it it just I think it might &gt;&gt; stop &gt;&gt; stop entangling them right because like RAG entangling them right because like RAG is just the process of augmenting. So ## [00:50:49 - 00:51:19] is just the process of augmenting. So the way MCP does retrieval though, you the way MCP does retrieval though, you can dial in. can dial in. &gt;&gt; Yeah, you can you can use MCP to find uh &gt;&gt; Yeah, you can you can use MCP to find uh to find context through an API, right? to find context through an API, right? Uh but but it's not meant to like house Uh but but it's not meant to like house your vector index. It's not meant to be your vector index. It's not meant to be a big reposi repository for uh for for a big reposi repository for uh for for uh piles of data, right? Like that isn't uh piles of data, right? Like that isn't what it's meant to solve. What it's what it's meant to solve. What it's meant to solve is uh I have an API and ## [00:51:19 - 00:51:52] meant to solve is uh I have an API and that API can give access to context and that API can give access to context and information and my LLM needs to know how information and my LLM needs to know how to be able to use that API. We're to be able to use that API. We're oversimplifying just a little bit here. Uh but this is the idea, right? Uh versus RAG which is about retrieving versus RAG which is about retrieving volumes of information. So I think volumes of information. So I think they're they're MCP they're they're MCP again model context protocol gives us a again model context protocol gives us a protocol to get context. Uh that doesn't protocol to get context. Uh that doesn't mean that it is uh replaces RAG or ## [00:51:52 - 00:52:22] mean that it is uh replaces RAG or should be thought of ahead of RAG. It's should be thought of ahead of RAG. It's in fact just more complex. So if you're in fact just more complex. So if you're getting started start with RAG. It's getting started start with RAG. It's very straightforward. dense vector very straightforward. dense vector retrieval plus uh hybrid uh plus sparse. So like BM25, we have a whole RA in 2025 So like BM25, we have a whole RA in 2025 video you can watch on it. Like that's video you can watch on it. Like that's still going to be the easiest way to to still going to be the easiest way to to get things done in today's world. M MCP get things done in today's world. M MCP though allows you to grow into more though allows you to grow into more complex use cases and attach things more ## [00:52:22 - 00:52:54] complex use cases and attach things more uh in in more fun ways, let's say. uh in in more fun ways, let's say. &gt;&gt; Okay. Okay. And I mean even RAG &gt;&gt; Okay. And I mean even RAG itself as we saw within the past two itself as we saw within the past two weeks, right? People are like, well, RAG weeks, right? People are like, well, RAG wasn't designed or this this type of wasn't designed or this this type of dense retrieval wasn't designed for all dense retrieval wasn't designed for all this. this. &gt;&gt; That's correct. &gt;&gt; It was designed for that &gt;&gt; It was designed for that &gt;&gt; specific thing there. And now now we're &gt;&gt; specific thing there. And now now we're in this space with MCP where like well in this space with MCP where like well MCP wasn't designed to replace RAG. MCP MCP wasn't designed to replace RAG. MCP was designed for a specific thing there. ## [00:52:54 - 00:53:23] was designed for a specific thing there. It was designed for that. it wasn't for It was designed for that. it wasn't for for this thing and so yeah it's quite for this thing and so yeah it's quite interesting to watch the evolution of interesting to watch the evolution of this um we got a clarifying question this um we got a clarifying question from from from &gt;&gt; uh Kon Singh Rouat &gt;&gt; uh Kon Singh Rouat isn't MCP just a framework built for RAG &gt;&gt; isn't MCP just a framework built for RAG applications Oh, applications I think this is I think this is &gt;&gt; isn't say that again I like I love that &gt;&gt; isn't say that again I like I love that question question question &gt;&gt; isn't MCP just a framework built for RAG ## [00:53:27 - 00:53:58] applications Oh, no. No, it isn't. But that's such a good no. question. &gt;&gt; Yeah. &gt;&gt; So, yeah, I mean, this is an interesting &gt;&gt; So, yeah, I mean, this is an interesting thing to think about, right? MCP was thing to think about, right? MCP was built to help models get the right built to help models get the right context from the right tools at the context from the right tools at the right time, right time, including data like straight just getting data from where it is. where it is. &gt;&gt; That's correct. Yeah. And this is part of I think what Yeah. And this is part of I think what the whole industry is go this is part of ## [00:53:58 - 00:54:29] the whole industry is go this is part of what even even in launching our cohort what even even in launching our cohort last night I was I was kind of last night I was I was kind of struggling with is is that we have these struggling with is is that we have these primary patterns we've dealt with but primary patterns we've dealt with but now we live in this world of context now we live in this world of context engineering and the context engineering engineering and the context engineering world is at the same level as MCP model world is at the same level as MCP model context protocol. So it's not just about context protocol. So it's not just about data it's not just about rags. It's not data it's not just about rags. It's not just about prompts. It's not just about just about prompts. It's not just about tools. It's tools. It's like it's about all of it like it's about all of it because we need all of it to to optimize ## [00:54:29 - 00:55:01] because we need all of it to to optimize our apps. That's right. And so, um, it's our apps. And so, um, it's a framework for for all of it. Not just a framework for for all of it. Not just &gt;&gt; Exactly. It's it's like &gt;&gt; Exactly. It's it's like it's like this it's like this MCP MCP MCP context engineering. Okay. Context context engineering. Context engineering. This is what we're calling engineering. This is what we're calling things, right? This is like the super things, right? This is like the super term, the umbrella term that sits above term, the umbrella term that sits above all of RAG and everything else. all of RAG and everything else. &gt;&gt; Context engineering, right? involves maneuvering context around a lot. Not just context from RAG, but context from just context from RAG, but context from agents, context from APIs, context from ## [00:55:01 - 00:55:32] agents, context from APIs, context from tools, context from cards, context from tools, context from cards, context from uh space, con, you know, from from other uh space, con, you know, from from other planes of existence. The idea is planes of existence. The idea is &gt;&gt; a way for us to standardize a lot of &gt;&gt; a way for us to standardize a lot of that stuff that isn't that stuff that isn't &gt;&gt; that isn't in a really useful &gt;&gt; that isn't in a really useful uh state to start with. Okay. So, so uh state to start with. So, so this is how I would implore people to to this is how I would implore people to to look at this. It's not already in a very look at this. It's not already in a very useful state, right? APIs and agents, useful state, right? APIs and agents, but like just just text from RAG already ## [00:55:32 - 00:56:03] but like just just text from RAG already in a useful state, right? We we we don't in a useful state, right? We we we don't really need the translation layer, but really need the translation layer, but for the other stuff, we sure do for the for the other stuff, we sure do for the whole world of context, we need we need whole world of context, we need we need a way to standardize, right? And so a way to standardize, right? And so that's what that protocol helps us to that's what that protocol helps us to do. We don't need to convert text files do. We don't need to convert text files to PDF to use them, but we might want to to PDF to use them, but we might want to convert other stuff to PDF to make it convert other stuff to PDF to make it more portable and useful. more portable and useful. &gt;&gt; That's right. &gt;&gt; Yeah. Okay. All right. &gt;&gt; That's exactly so. &gt;&gt; Love it. Okay. All right. Yeah, good &gt;&gt; Love it. Yeah, good discussion. I agree, Edgar. Thanks, man. ## [00:56:03 - 00:56:34] discussion. I agree, Edgar. Thanks, man. Uh, so next we got What do we got up Uh, so next we got What do we got up next after connectors? We we got agents. next after connectors? We're going to talk search. We're going We're going to talk search. We're going to talk tools. We're going to talk function calling. We're going to do this next week. Um, within this, we're going next week. Um, within this, we're going to open up the Agents SDK. Um, can you to open up the Agents SDK. Um, can you give people a little bit of a of an give people a little bit of a of an insight into Agents SDK versus responses insight into Agents SDK versus Responses API for session three? API for session three? &gt;&gt; We're stepping up a level. We're stepping up one layer of abstraction. We're now going to use something that ## [00:56:34 - 00:57:04] We're now going to use something that uses Responses API. uses Responses API. &gt;&gt; Okay. So, but similarly we can think &gt;&gt; Okay. So, but similarly we can think about it as about it as if we're building let's say with if we're building let's say with langraph we could use we could connect langraph we could use we could connect to Agents SDK in the same way we're to Agents SDK in the same way we're connecting to the Responses API even connecting to the Responses API even though it's at a higher layer of though it's at a higher layer of abstraction it's not it's not the core abstraction it's not it's not the core necessarily of what we're building with necessarily of what we're building with although maybe it could be although maybe it could be &gt;&gt; we're connecting similarly okay &gt;&gt; we're connecting similarly okay &gt;&gt; that's right &gt;&gt; that's right &gt;&gt; all right whiz thanks for your insights ## [00:57:04 - 00:57:36] &gt;&gt; all right whiz thanks for your insights as always man uh great QA period And as always man uh great QA period And thank you everybody for joining us for thank you everybody for joining us for uh session two here at AI Makerspace. We uh session two here at AI Makerspace. We are on a mission to create the world's are on a mission to create the world's leading community for people like you leading community for people like you who want to build, ship, and share who want to build, ship, and share production LLM applications. Wherever production LLM applications. Wherever the context comes from, wherever that the context comes from, wherever that context will lead us to. Um join our context will lead us to. Um join our community. It's full of cool, competent community. It's full of cool, competent folks like many in the Discord today and ## [00:57:36 - 00:58:07] folks like many in the Discord today and many in the YouTube live chat today as many in the YouTube live chat today as well. Uh we launched another cohort of well. Uh we launched another cohort of the AI engineering boot camp. Stay tuned the AI engineering boot camp. Stay tuned for more on upcoming for more on upcoming that was just last night actually. So that was just last night actually. So shout out to any cohort 8 folks in the shout out to any cohort 8 folks in the audience today. Um, we are going to be audience today. Um, we are going to be talking about demo day within good time talking about demo day within good time and we've got a new special upcoming and we've got a new special upcoming course that we're excited to announce course that we're excited to announce next week. So stay tuned for that. If next week. If you need any help with your team before you need any help with your team before the end of the year, uh, reach out. We ## [00:58:07 - 00:58:38] the end of the year, uh, reach out. We can certainly think about how we can can certainly think about how we can provide provide provide some real value whether it's from direct some real value whether it's from direct training or actually getting our entire training or actually getting our entire network of solo consultants that we've network of solo consultants that we've trained and certified to help you out trained and certified to help you out directly. So with that, thank you so directly. So with that, thank you so much everybody for joining us again for much everybody for joining us again for another YouTube live session. Um it's another YouTube live session. Um it's been a pleasure. Next week, session been a pleasure. Next week, session three, agents three, agents tools, function calling, Agents SDK. ## [00:58:38 - 00:59:10] tools, function calling, Agents SDK. We're going to step it up one more We're going to step it up one more layer. Uh stay tuned and uh in the layer. Uh stay tuned and uh in the meantime, keep building, shipping, and meantime, keep building, shipping, and sharing and we'll do the same. Thanks sharing and we'll do the same. Thanks for coming today, guys. See you next for coming today, guys. See you next week. week. But Greg, how do they get started But Greg, how do they get started building shipping and sharing? Getting building shipping and sharing? Getting started is the hardest part like it was started is the hardest part like it was for you and for me. You can get started for you and for me. You can get started with the AI engineer challenge and you with the AI engineer challenge and you can share it in the build ship share can share it in the build ship share channel on discord links in the channel on discord links in the description. What if they want to description. What if they want to accelerate their ability? Well, we ## [00:59:10 - 00:59:32] accelerate their ability? Well, we actually have a 10-week intensive boot actually have a 10-week intensive boot camp where you learn to build ship and camp where you learn to build ship and share production ready LLM applications share production ready LLM applications every single week. The AI engineering every single week. The AI engineering boot camp cohort starting soon. We'll boot camp cohort starting soon. We'll have you building, shipping, and sharing have you building, shipping, and sharing like a legend like a legend &gt;&gt; in no time. We'll see you in class.